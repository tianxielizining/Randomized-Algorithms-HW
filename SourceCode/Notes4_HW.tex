\section{Chernoff Bound: Measure Concentration}
\begin{theorem}[Basic Chernoff Bound]
    Suppose $Y$ is the sum of $n$ independent $\{0,1\}$-random variables $X_i$'s such that for each $i$, $\Pr[X_i=1]=p$. 
    Let $\mu :=E[Y]=np$. Then, for $0<\epsilon<1$,
    \begin{align}
       \nonumber \Pr[|Y-E[Y]|\le \epsilon E[Y]]\le 2\exp({-\frac{1}{3}\epsilon^2np})
    \end{align}
\end{theorem}
\subsection{Homework}
\blue{1. \textbf{The Other Half of Chernoff.} Suppose $X_0, X_1, ..., X_{n-1}$ are independent $\{0,1\}$-random
variables, each having expectation $p$. Let $Y := \sum_{i}X_i$ and $\mu := E[Y]$. Using the method of
moment generating function, prove the following.
\begin{align}
    \nonumber \text{For all}~ \epsilon >0 \text{, }\Pr[Y-\mu \ge\epsilon \mu] \le (\frac\exp({\epsilon}){(1+\epsilon)^{1+\epsilon}})^{\mu}
\end{align}}
\begin{proof}
Set $t>0$, then we have:
\begin{align}
    \nonumber \Pr[Y-\mu \ge \epsilon \mu] &=\Pr[\sum_{i}X_i-\mu \ge \epsilon \mu]\\
    \nonumber &=\Pr[t\sum_{i}X_i \ge t\mu(1+\epsilon)]\\
    \nonumber &=\Pr[\exp({t\sum_{i}X_i } )\ge \exp({t\mu(1+\epsilon)})]
\end{align}
By Markov's inequality, we have:
\begin{align}
    \nonumber \Pr[\exp({t\sum_{i}X_i }) \ge \exp({t\mu(1+\epsilon)})] \le \frac{E[\exp({t\sum_{i}X_i })]}{\exp({t\mu(1+\epsilon)})}
\end{align}
Since $X_i$'s are independent, we have:
\begin{align}
    \nonumber E[\exp({t\sum_{i}X_i })]=\prod_{i}E[\exp({tX_i})]
\end{align}
By definition, for each $\Pr[X_i=1]=p$, $\Pr[X_i=0]=1-p$, and the fact that for all real numbers $x$, $1+x\le \exp(x)$, we have:
\begin{align}
    \nonumber E[\exp({tX_i})]=\exp(t)\cdot p+\exp(0) \cdot (1-p)=1+p(\exp(t)-1) \le \exp({p(\exp(t)-1)})
\end{align}
Then,
\begin{align}
    \nonumber \prod_{i}E[\exp({tX_i})]\le \exp({np(\exp(t)-1)})
\end{align}
Thus, we have:
\begin{align}
    \nonumber \Pr[Y-\mu \ge \epsilon \mu] \le \frac{\exp({np(\exp(t)-1)})}{\exp({t\mu(1+\epsilon)})}=\exp({np(\exp(t)-1)-\mu t(1+\epsilon)})=\exp(\mu(\exp(t)-(1+\epsilon)t-1))
\end{align}
Set $g(t)=\mu(\exp(t)-(1+\epsilon)t-1)$. We aim to find the value $t$ that minimizes the function $g(t)$.
We have $g'(t)=\mu(\exp(t)-(1+\epsilon))$ and $g''(t)=\mu \exp(t) >0$. It follows that function $g(t)$ attains its minimum when $g'(t) = 0$.
Check that $t>0$, so we can set $t=\ln{(1+\epsilon)}$.
And $g(\ln{(1+\epsilon)})=\mu(1+\epsilon-(1+\epsilon)\ln{(1+\epsilon)}-1)=\mu(\epsilon-(1+\epsilon)\ln(1+\epsilon))$.
Thus,
\begin{align}
    \nonumber \Pr[Y-\mu \ge \epsilon \mu] &\le \exp({\mu(\epsilon-(1+\epsilon)\ln(1+\epsilon))})\\
    \nonumber &=(\frac{\exp(\epsilon)}{\exp({(1+\epsilon)\ln(1+\epsilon)})})^{\mu}\\
    \nonumber &=(\frac{\exp(\epsilon)}{(1+\epsilon)^{1+\epsilon}})^{\mu}
\end{align}
\end{proof}
\noindent \blue{2. \textbf{$n$ Balls into $n$ Bins (Revisited).} 
Using the Chernoff Bound from the previous question, we can obtain a better bound for the balls and bins problem. 
Suppose $n$ balls are thrown independently and uniformly at random into $n$ bins. 
Let $Y_1$ be the number of balls in the first bin. Assume that $n$ is large enough, say $n\ge 100$.\\
(a) Find a number $N$ in terms of $n$ such that $\Pr[Y_1\ge N]\le \frac{1}{n^2}$. 
Please give the exact form and do not use big $O$ notation for this part of the question.
}\\
\begin{claim}\label{1}
    For all $n> e$, $\ln \ln n  \ge 2\ln \ln \ln n$.
\end{claim}
\begin{proof}
    Let $t=\ln \ln n$, then it turns to proof $t\ge 2\ln t$ when $t> 0$.
    Let $f(t)=t-2\ln t$ where $t> 0$.
    $f'(t)=1-\frac{2}{t}$. 
    When $t\in (0,2)$, $f'(t)<0$.
    When $t=2$, $f'(t)=0$.
    When $t>2$, $f'(t)>0$.
    It follows that function $f(t)$ attains its minimum when $t=2$ and $f(2)=2-2\ln 2>0$.
    Then, $t\ge 2\ln t$ when $t>0$.
\end{proof}
\noindent By definition, $E[Y_1]=\frac{1}{n}\cdot n =1$.
By Chernoff Bound, we have,
\begin{align}
    \nonumber \Pr[Y_1\ge 1+\epsilon] \le \frac{\exp(\epsilon)}{(1+\epsilon)^{1+\epsilon}}
\end{align}
Since we have to satisfy $\Pr[Y_1\ge N] \le \frac{1}{n^2}$, we can set $N:=1+\epsilon$ and have,
\begin{align}
    \nonumber \frac{\exp(\epsilon)}{(1+\epsilon)^{1+\epsilon}}&\le \frac{1}{n^2}\\
    \nonumber \frac{\exp({N-1})}{N^N}&\le \frac{1}{n^2}\\
    \nonumber \frac{N^N}{\exp({N-1})}&\ge n^2\\
    \nonumber  \ln (\frac{N^N}{\exp({N-1})})&\ge \ln (n^2)\\
    \nonumber N \ln N - (N-1) &\ge 2\ln n \\
    \nonumber N(\ln N -1) & \ge 2\ln n +1
\end{align}
Set $N:=\frac{\lambda \ln n}{\ln \ln n}$, we need to satisfy:
\begin{align}
    \nonumber \frac{\lambda \ln n}{\ln \ln n}(\ln (\frac{\lambda \ln n}{\ln \ln n})-1) \ge 2\ln n +1\\
    \nonumber \frac{\lambda \ln n}{\ln \ln n}(\ln \lambda +\ln\ln n -\ln \ln \ln n -1) \ge 2\ln n +1
\end{align}

%Since $\ln \ln n -\ln \ln \ln n \ge \frac{\ln \ln n}{2}$, 
By Claim \ref{1},
we have,
\begin{align}
    \nonumber \frac{\lambda \ln n}{\ln \ln n}(\ln \lambda +\ln n -\ln \ln \ln n -1) &\ge \frac{\lambda \ln n}{\ln \ln n}(\ln \lambda -1 +\frac{\ln \ln n}{2})\\
    \nonumber &=\frac{\lambda \ln n}{\ln \ln n}\cdot \frac{\ln\ln n}{2}+\frac{\lambda \ln n}{\ln \ln n}\cdot(\ln\lambda-1)\\
    \nonumber &=\frac{\lambda \ln n}{2} +\frac{\lambda \ln n}{\ln \ln n}\cdot(\ln\lambda-1)
\end{align}
Set $\lambda \ge e$, then, $\ln\lambda-1 \ge 0$. Thus,
\begin{align}
    \nonumber \frac{\lambda \ln n}{\ln \ln n}(\ln \lambda +\ln n -\ln \ln \ln n -1) &\ge \frac{\lambda \ln n}{2}
\end{align}
Then we have to satisfy:
$\frac{\lambda \ln n}{2} \ge 2\ln n +1$ and $\lambda \ge e$ at the same time. So we have:
\begin{align}
    \nonumber \lambda \ge \frac{4\ln n +2}{\ln n}
\end{align}
Since $N=\frac{\lambda \ln n}{\ln \ln n}$, we have:
\begin{align}
    \nonumber N&\ge \frac{4\ln n +2}{\ln n}\cdot \frac{\ln n}{\ln \ln n}\\
    \nonumber &= \frac{4\ln n +2}{\ln \ln n}
\end{align}
\noindent\blue{(b) Show that with probability at least $1-\frac{1}{n}$, no bin contains more than $\Theta(\frac{\log n}{\log\log n})$ balls.}\\
Let $B_1$ be the event that there exists a bin that with more than $\frac{4\ln n +2}{\ln \ln n}$ balls.
By union bound, we have $\Pr[B_1]\le\frac{1}{n^2}\cdot n=\frac{1}{n}$.
Let $B_2$ be the event that no bin contains more than $\frac{4\ln n +2}{\ln \ln n}$ balls.
Then, $\Pr[B_2]\ge 1-\frac{1}{n}$.\\
Since $\frac{4\ln n +2}{\ln \ln n}=\Theta(\frac{\log n}{\log\log n})$, we have that with probability at least $1-\frac{1}{n}$, no bin contains more than $\Theta(\frac{\log n}{\log\log n})$ balls.